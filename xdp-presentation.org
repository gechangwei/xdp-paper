#+TITLE: The eXpress Data Path
#+AUTHOR: Toke Høiland-Jørgensen
#+EMAIL: toke@toke.dk
#+REVEAL_THEME: white
#+REVEAL_TRANS: slide
#+REVEAL_MARGIN: 0
#+REVEAL_EXTRA_JS: { src: './reveal.js/js/custom-kau.js'}
#+REVEAL_MATHJAX_URL: ./reveal.js/js/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML
#+OPTIONS: reveal_center:nil reveal_control:t reveal_history:nil
#+OPTIONS: reveal_width:1600 reveal_height:1000
#+OPTIONS: ^:nil tags:nil toc:nil num:nil ':t

* Outline
:PROPERTIES:
:reveal_extra_attr: class="mid-slide"
:END:

- Challenges with high-speed packet processing
- XDP design
- Performance evaluation
- Example applications
- Conclusion

* High-Speed Packet Processing is Hard

- Millions of packets per second
  - 10 Gbps: 14.8Mpps / 67.5 ns per packet
  - 100 Gbps: 148Mpps / 6.75 ns per packet

Operating system stacks are /*too slow*/ to keep up

** Previous solutions

  - Kernel bypass - move hardware to userspace
    - /High performance/, but /*hard to integrate*/ with the system

  - Fast-path frame-to-userspace solutions (Netmap etc)
    - /Kernel in control/, but /*lower performance*/

  - Custom in-kernel modules (e.g., Open vSwitch)
    - /Avoids context switching/, but is a /*maintenance burden*/

XDP: Move processing *into the kernel* instead

* XDP: Benefits
- Integrated with the kernel; driver retains control of hardware
- Can selectively use kernel stack features
- Stable API
- No packet re-injection needed
- Transparent to the host
- Dynamically re-programmable
- Doesn't need a full CPU core

** XDP: Overall design

#+ATTR_HTML: :class figure-right :style height: 900px; position: absolute; top: 50px; right: 50px;
[[file:figures/xdp-diagram-cut.svg]]

- The XDP driver hook
- The eBPF virtual machine
- BPF maps
- The eBPF verifier

** The eBPF virtual machine                                       :noexport:
- In-kernel virtual machine
- Extended version of BPF bytecode
- JIT-compiles to most kernel architectures
- In-kernel verifier ensures safety

** The eBPF verifier                                              :noexport:
- Static analysis of programs /at load time/
- Walks the instruction DAG to ensure:
  - No loops
  - No invalid instructions
  - Only safe memory access

Overall objective: *Protect the kernel*

** XDP program flow
:PROPERTIES:
:reveal_extra_attr: class="extra-slide"
:END:

#+ATTR_HTML: :class figure-bg :style position: relative; left: -2.5em;
[[file:figures/xdp-execution-diagram.svg]]

#+BEGIN_NOTES
 Execution flow of a typical XDP program. When a packet arrives, the program starts by parsing packet headers to extract the information it will react on. It then reads or updates metadata from one of several sources. Finally, a packet can be rewritten and a final verdict for the packet is determined. The program can alternate between packet parsing, metadata lookup and rewriting, all of which are optional. The final verdict is given in the form of a program return code.
#+END_NOTES

** Example XDP program

#+REVEAL_HTML: <div class="tiny-col">
#+begin_src C
/* map used to count packets; key is IP protocol,
   value is pkt count */
struct bpf_map_def SEC("maps") rxcnt = {
	.type = BPF_MAP_TYPE_PERCPU_ARRAY,
	.key_size = sizeof(u32),
	.value_size = sizeof(long),
	.max_entries = 256,
};

/* swaps MAC addresses using direct packet data access */
static void swap_src_dst_mac(void *data)
{
	unsigned short *p = data;
	unsigned short dst[3];
	dst[0] = p[0];
	dst[1] = p[1];
	dst[2] = p[2];
	p[0]   = p[3];
	p[1]   = p[4];
	p[2]   = p[5];
	p[3]   = dst[0];
	p[4]   = dst[1];
	p[5]   = dst[2];
}

static int parse_ipv4(void *data, u64 nh_off, void *data_end)
{
	struct iphdr *iph = data + nh_off;
	if (iph + 1 > data_end)
		return 0;
	return iph->protocol;
}

#+END_src
#+REVEAL_HTML: </div>
#+REVEAL_HTML: <div class="tiny-col" style="position: relative; top: -3em;">
#+BEGIN_src C

SEC("xdp1") /* marks main eBPF program entry point */
int xdp_prog1(struct xdp_md *ctx)
{
	void *data_end = (void *)(long)ctx->data_end;
	void *data = (void *)(long)ctx->data;
	struct ethhdr *eth = data; int rc = XDP_DROP;
	long *value; u16 h_proto; u64 nh_off; u32 ipproto;

	nh_off = sizeof(*eth);
	if (data + nh_off > data_end)
		return rc;

	h_proto = eth->h_proto;

	/* check VLAN tag; could be repeated to support double-tagged VLAN */
	if (h_proto == htons(ETH_P_8021Q) || h_proto == htons(ETH_P_8021AD)) {
		struct vlan_hdr *vhdr;

		vhdr = data + nh_off;
		nh_off += sizeof(struct vlan_hdr);
		if (data + nh_off > data_end)
			return rc;
		h_proto = vhdr->h_vlan_encapsulated_proto;
	}

	if (h_proto == htons(ETH_P_IP))
		ipproto = parse_ipv4(data, nh_off, data_end);
	else if (h_proto == htons(ETH_P_IPV6))
		ipproto = parse_ipv6(data, nh_off, data_end);
	else
		ipproto = 0;

	/* lookup map element for ip protocol, used for packet counter */
	value = bpf_map_lookup_elem(&rxcnt, &ipproto);
	if (value)
		*value += 1;

	/* swap MAC addrs for UDP packets, transmit out this interface */
	if (ipproto == IPPROTO_UDP) {
		swap_src_dst_mac(data);
		rc = XDP_TX;
	}
	return rc;
}
#+end_src
#+REVEAL_HTML: </div>

* Performance benchmarks

- Benchmark against DPDK
  - Establishes baseline performance
  - Simple tests
    - Packet drop performance
    - CPU usage
    - Packet forwarding performance

All tests are with /64 byte packets/ - measuring *Packets Per Second (PPS)*.

** Packet drop performance
:PROPERTIES:
:reveal_extra_attr: class="extra-slide"
:END:

[[file:figures/drop-test.svg]]

#+BEGIN_NOTES
 Packet drop performance. DPDK uses one core for control tasks, so only 5 are available for packet processing.
#+END_NOTES

** CPU usage in drop test
:PROPERTIES:
:reveal_extra_attr: class="extra-slide"
:END:

[[file:figures/drop-cpu.svg]]

#+BEGIN_NOTES
 CPU usage in the drop scenario. Each line stops at the method's maximum processing capacity. The DPDK line continues at 100\% up to the maximum performance shown in Figure~\reffig:drop-test.
#+END_NOTES

** Packet forwarding throughput
:PROPERTIES:
:reveal_extra_attr: class="extra-slide"
:END:

[[file:figures/redirect-test.svg]]

#+BEGIN_NOTES
 Packet forwarding throughput. Sending and receiving on the same interface takes up more bandwidth on the same PCI port, which means we hit the PCI bus limit at 70 Mpps.
#+END_NOTES

** Packet forwarding latency

#+REVEAL_HTML: <div class="spacer"></div>

#+REVEAL_HTML: <table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
#+REVEAL_HTML: <thead>
#+REVEAL_HTML: <tr>
#+REVEAL_HTML: <th>&#xa0;</th>
#+REVEAL_HTML: <th colspan="2" align="center">Average</th>
#+REVEAL_HTML: <th colspan="2" align="center">Maximum</th>
#+REVEAL_HTML: <th colspan="2" align="center">\(< 10 \mu s\)</th>
#+REVEAL_HTML: </tr>
#+REVEAL_HTML: <tr>
#+REVEAL_HTML: <th>&#xa0;</th>
#+REVEAL_HTML: <th>100 pps</th>
#+REVEAL_HTML: <th>1 Mpps</th>
#+REVEAL_HTML: <th>100 pps</th>
#+REVEAL_HTML: <th>1 Mpps</th>
#+REVEAL_HTML: <th>100 pps</th>
#+REVEAL_HTML: <th>1 Mpps</th>
#+REVEAL_HTML: </tr>
#+REVEAL_HTML: </thead>
#+REVEAL_HTML: <tbody>
#+REVEAL_HTML: <tr>
#+REVEAL_HTML: <td>XDP</td>
#+REVEAL_HTML: <td>\(82 \mu s\)</td>
#+REVEAL_HTML: <td>\(7 \mu s\)</td>
#+REVEAL_HTML: <td>\(272 \mu s\)</td>
#+REVEAL_HTML: <td>\(202 \mu s\)</td>
#+REVEAL_HTML: <td>\(0 \%\)</td>
#+REVEAL_HTML: <td>\(98.1 \%\)</td>
#+REVEAL_HTML: </tr>
#+REVEAL_HTML: <tr>
#+REVEAL_HTML: <td>DPDK</td>
#+REVEAL_HTML: <td>\(2 \mu s\)</td>
#+REVEAL_HTML: <td>\(3 \mu s\)</td>
#+REVEAL_HTML: <td>\(161 \mu s\)</td>
#+REVEAL_HTML: <td>\(189 \mu s\)</td>
#+REVEAL_HTML: <td>\(99.5 \%\)</td>
#+REVEAL_HTML: <td>\(99.0 \%\)</td>
#+REVEAL_HTML: </tr>
#+REVEAL_HTML: </tbody>
#+REVEAL_HTML: </table>

* Application proof-of-concept

- Shows feasibility of three applications:

  - Software router
  - DDoS protection system
  - Layer-4 load balancer

/*Not*/ a benchmark against state-of-the-art implementations

** Software routing performance
:PROPERTIES:
:reveal_extra_attr: class="extra-slide"
:END:
#+REVEAL_HTML: <div class="spacer"></div>

[[file:figures/router-fwd.svg]]

#+BEGIN_NOTES
 Software routing performance. Since the performance scales linearly with the number of cores, only the results for a single core are shown.
#+END_NOTES

** DDoS protection
:PROPERTIES:
:reveal_extra_attr: class="extra-slide"
:END:

[[file:figures/ddos-test.svg]]

** Load balancer performance

#+REVEAL_HTML: <div class="spacer"></div>


| CPU Cores    |   1 |    2 |    3 |    4 |    5 | 6     |
|--------------+-----+------+------+------+------+-------|
| XDP (Katran) | 5.2 | 10.1 | 14.6 | 19.5 | 23.4 | 29.3  |
| Linux (IPVS) | 1.2 |  2.4 |  3.7 |  4.8 |  6.0 | 7.3   |

#+REVEAL_HTML: <div class="spacer"></div>

#+REVEAL_HTML: <div class="small">
Based on the [[https://github.com/facebookincubator/katran][Katran load balancer]] (open sourced by Facebook).
#+REVEAL_HTML: </div>
* Summary

XDP:

- Integrates programmable packet processing *into the kernel*
- Combines /speed/ with /flexibility/
- Is supported by the Linux kernel community
- Is /*already used*/ in high-profile production use cases

#+ATTR_HTML: :style width: 150px; position: absolute; bottom: 50px; right: 50px;
[[file:figures/artifacts_evaluated_reusable.jpg]]
* Acknowledgements

XDP has been developed *over a number of years* by the /Linux networking
community/. Thanks to everyone involved; in particular, to:

- /*Alexei Starovoitov*/ for his work on the eBPF VM and verifier
- /*Björn Töpel*/ and /*Magnus Karlsson*/ for their work on AF_XDP

Also thanks to our anonymous reviewers, and to our shepherd Srinivas Narayana
for their helpful comments on the paper

* Notes etc                                                        :noexport:

# Local Variables:
# org-reveal-title-slide: "<h1 class=\"title\">%t</h1><h2 class=\"subtitle\">Fast Programmable Packet Processing in the Operating System Kernel</h2>
# <h2 class=\"author current\">Toke Høiland-Jørgensen (Karlstad University)</h2>
# <h2 class=\"author\">Jesper Dangaard Brouer (Red Hat)</h2>
# <h2 class=\"author\">Daniel Borkmann (Cilium.io)</h2>
# <h2 class=\"author\">John Fastabend (Cilium.io)</h2>
# <h2 class=\"author\">Tom Herbert (Quantonium Inc)</h2>
# <h2 class=\"author\">David Ahern (Cumulus Networks)</h2>
# <h2 class=\"author\">David Miller (Red Hat)</h2>
# <h3>CoNEXT '18<br/>Heraklion, Greece, Dec 2018</h3>"
# End:
