* Layer3 routing test
We use the xdp_fwd routing table helper for these benchmarks, compare with
baseline Linux.

: ./xdp_fwd -D ens1f1

Normal Linux forwarding also skip the rules lookup, so that corresponds to
xdp_fwd direct mode. But only if no rules have *ever* been installed since the
machine was rebooted. Removing the rules does *not* get it back in the optimised
lookup mode.

Measure throughput with ethtool_stats.pl (rx_xdp_redirect /sec). Adapter RX ring
set to different values:

: ethtool -G ens1f1 rx X

** Full routing table test

To run the full routing table test, a different trex script is used,
udp_multi_ip.py, which randomises the destination IPs. For this script the
stream_count parameter sets the number of dest IPs, and port_count sets the
number of dest UDP ports (for flow rule scaling at the receiver).

Running:

: start -f stl/udp_multi_ip.py -t packet_len=64,stream_count=20000,port_count=1 --port 0 -m 100%

Using larger numbers for stream_count takes a long time to setup! It's not
efficient! :)

20000 is the max number of streams supported by TRex, it seems. With this, TRex
can generate ~17Mpps on my machine. I also needed to increase the number of
mbufs in /etc/trex_cfg.yaml:

#+begin_example
  memory:
    mbuf_64: 200000
#+end_example

With 4000 streams, I can generate ~35Mpps, which is a bit above the maximum
forwarding rate with XDP, so that's what I'm going with.

Note that the randomly generated IPs may hit holes in the routing table, so make
sure there's either a default route or a covering route going to the same
destination. I use:

: ip r add 0.0.0.0/1 via 10.70.2.2
: ip r add 128.0.0.0/1 via 10.70.2.2

These are more specific than the default route (so that is still there if I want
to go out onto the internet), but covers everything.

** Data

#+NAME: fwd_data
| Cores | Linux (single route) | Linux (full table) | XDP (single route rxring 1024) | XDP (single route rxring 512) | XDP (single route rxring 256) | XDP (full table rxring 256) |
|-------+----------------------+--------------------+--------------------------------+-------------------------------+-------------------------------+-----------------------------|
|     1 |              1739672 |            1373550 |                        5211477 |                       5357696 |                       5355583 |                     3446336 |
|     2 |              3370584 |            2746704 |                       10239315 |                      10570550 |                      10652337 |                     7016974 |
|     3 |              4976559 |            3958725 |                       13731038 |                      15722609 |                      15937977 |                    10641333 |
|     4 |              6488625 |            5295366 |                       16393261 |                      20192727 |                      21353932 |                    14408549 |
|     5 |              7848970 |            6451024 |                       19364863 |                      23225298 |                      26056821 |                    17870856 |
|     6 |              9285971 |            7809312 |                       22852944 |                      25916115 |                      30416566 |                    21227330 |

The performance difference in nanosec between single and full-table:

Linux routing:
 (1/1739672-1/1373550)*10^9 = -153.22 ns

XDP-routing:
 (1/5357696-1/3446336)*10^9 = -103.52 ns

This overhead of the full-table is significantly larger than expected.
The overhead different was expected in the 10-15 ns range, based on
Vincent Bernat's blogposts:

 https://vincent.bernat.im/en/blog/2017-performance-progression-ipv4-route-lookup-linux

See:
 https://d1g3mdmxf8zbo9.cloudfront.net/images/linux/lpc-trie-perf-recent-v2.svg

#+BEGIN_SRC ipython :session :exports both :results raw drawer :var data=fwd_data
d = list(map(lambda x: x/10**6, [data[0][2], data[0][1], data[0][6], data[0][5]]))
labels = ["Linux (full table)", "Linux (single route)",
          "XDP (full table)","XDP (single route)"]

#plt.plot(d[:,0], d[:,5]/10**6, marker='o', label="XDP (single route)")
#plt.plot(d[:,0], d[:,6]/10**6, marker='s', label="XDP (full table)")
#plt.plot(d[:,0], d[:,1]/10**6, marker='^', label="Linux (single route)")
#plt.plot(d[:,0], d[:,2]/10**6, marker='x', label="Linux (full table)")
plt.barh(range(len(d)), d, tick_label=labels, color=["#1b9e77", "#1b9e77", "#d95f02", "#d95f02"])
plt.xlabel("Mpps (single core)")
plt.gcf().set_figheight(3)
plt.savefig(BASEDIR+"/figures/router-fwd.pdf", bbox_inches='tight')
plt.show()
#+END_SRC

#+RESULTS:
:results:
# Out[50]:
[[file:./obipy-resources/A96bbg.svg]]
:end:


* Jesper reproduceing Layer3 routing test

Setup fake "next-hop" gateway via a fake ARP/neigh entry.

: ip neigh replace 198.18.100.66 dev mlx5p2 lladdr 00:11:22:33:00:66

Ingress device is: mlx5p1

: 198.18.1.0/24 dev mlx5p1 proto kernel scope link src 198.18.1.1 metric 100

Engress device is: mlx5p2

: 198.18.100.0/24 dev mlx5p2 proto kernel scope link src 198.18.100.2 metric 100

Route everything through more specific route trick:

: ip r add 0.0.0.0/1   via 198.18.100.66
: ip r add 128.0.0.0/1 via 198.18.100.66

** Installing full route table

: wget https://kau.toke.dk/xdp/all-bgp-routes-20180419.txt.xz
: xzcat all-bgp-routes-20180419.txt.xz | sudo ./read_route_table.sh 198.18.100.66

#+BEGIN_EXAMPLE
$ ip r | wc -l
752147
#+END_EXAMPLE

** Starting xdp_fwd program

 sudo ./xdp_fwd -D mlx5p1 mlx5p2 i40e1

** Single queue performance

: ethtool -L mlx5p1 combined 1

With stream_count=20000 which is random dest-IPs:

: trex>start -f /home/jbrouer/git/xdp-paper/benchmarks/udp_multi_ip02.py --port 0 -m 100% -t packet_len=64,stream_count=20000,port_count=1

Performance RX-ring size 1024, stream_count=20000:
 Ethtool(mlx5p1  ) stat:      3809490 (      3,809,490) <= rx_xdp_redirect /sec
 Ethtool(mlx5p2  ) stat:      3815161 (      3,815,161) <= tx_xdp_xmit /sec

Full ethtool_stats.pl output:

#+BEGIN_EXAMPLE
 Show adapter(s) (mlx5p1 mlx5p2) statistics (ONLY that changed!)
 Ethtool(mlx5p1  ) stat:        59523 (         59,523) <= ch0_poll /sec
 Ethtool(mlx5p1  ) stat:        59523 (         59,523) <= ch_poll /sec
 Ethtool(mlx5p1  ) stat:      3809487 (      3,809,487) <= rx0_cache_empty /sec
 Ethtool(mlx5p1  ) stat:      3809489 (      3,809,489) <= rx0_xdp_redirect /sec
 Ethtool(mlx5p1  ) stat:     24691870 (     24,691,870) <= rx_64_bytes_phy /sec
 Ethtool(mlx5p1  ) stat:   1580272579 (  1,580,272,579) <= rx_bytes_phy /sec
 Ethtool(mlx5p1  ) stat:      3809508 (      3,809,508) <= rx_cache_empty /sec
 Ethtool(mlx5p1  ) stat:     20882293 (     20,882,293) <= rx_out_of_buffer /sec
 Ethtool(mlx5p1  ) stat:     24691763 (     24,691,763) <= rx_packets_phy /sec
 Ethtool(mlx5p1  ) stat:   1580386044 (  1,580,386,044) <= rx_prio0_bytes /sec
 Ethtool(mlx5p1  ) stat:     24693509 (     24,693,509) <= rx_prio0_packets /sec
 Ethtool(mlx5p1  ) stat:   1481501437 (  1,481,501,437) <= rx_vport_unicast_bytes /sec
 Ethtool(mlx5p1  ) stat:     24691704 (     24,691,704) <= rx_vport_unicast_packets /sec
 Ethtool(mlx5p1  ) stat:      3809490 (      3,809,490) <= rx_xdp_redirect /sec
 Ethtool(mlx5p2  ) stat:        59443 (         59,443) <= ch0_arm /sec
 Ethtool(mlx5p2  ) stat:        59443 (         59,443) <= ch0_events /sec
 Ethtool(mlx5p2  ) stat:        59442 (         59,442) <= ch0_poll /sec
 Ethtool(mlx5p2  ) stat:        59443 (         59,443) <= ch_arm /sec
 Ethtool(mlx5p2  ) stat:        59443 (         59,443) <= ch_events /sec
 Ethtool(mlx5p2  ) stat:        59443 (         59,443) <= ch_poll /sec
 Ethtool(mlx5p2  ) stat:        59612 (         59,612) <= tx0_xdp_cqes /sec
 Ethtool(mlx5p2  ) stat:      3815163 (      3,815,163) <= tx0_xdp_xmit /sec
 Ethtool(mlx5p2  ) stat:    244171295 (    244,171,295) <= tx_bytes_phy /sec
 Ethtool(mlx5p2  ) stat:      3815169 (      3,815,169) <= tx_packets_phy /sec
 Ethtool(mlx5p2  ) stat:    244172134 (    244,172,134) <= tx_prio0_bytes /sec
 Ethtool(mlx5p2  ) stat:      3815185 (      3,815,185) <= tx_prio0_packets /sec
 Ethtool(mlx5p2  ) stat:    228910118 (    228,910,118) <= tx_vport_unicast_bytes /sec
 Ethtool(mlx5p2  ) stat:      3815169 (      3,815,169) <= tx_vport_unicast_packets /sec
 Ethtool(mlx5p2  ) stat:        59612 (         59,612) <= tx_xdp_cqes /sec
 Ethtool(mlx5p2  ) stat:      3815161 (      3,815,161) <= tx_xdp_xmit /sec
#+END_EXAMPLE

T-rex command variable stream_count=XXX :

: start -f /home/jbrouer/git/xdp-paper/benchmarks/udp_multi_ip02.py --port 0 -m 100% -t packet_len=64,stream_count=XXX,port_count=1

#+NAME: fwd_data_streams
| Cores | stream_count (IPs) | XDP (full table rxring 512) |
|-------+--------------------+-----------------------------|
|     1 |                  1 |                     5767181 |
|     1 |                  2 |                     5550183 |
|     1 |                  4 |                     5425764 |
|     1 |                 10 |                     5284065 |
|     1 |                 20 |                     5051856 |
|     1 |                 50 |                     4966618 |
|     1 |                100 |                     4726404 |
|     1 |                200 |                     4615598 |
|     1 |                500 |                     4263151 |
|     1 |               1000 |                     4060469 |
|     1 |               2500 |                     3899194 |
|     1 |               5000 |                     3888937 |
|     1 |              10000 |                     3865736 |
|     1 |              20000 |                     3850132 |
|       |                    |                             |


Watching perf stat during test, shows that the performance issue, is
related to increase number of branch-misses and reduced insn per cycle
efficiency.

: perf stat -C0 -e cycles -e  instructions -e cache-references -e cache-misses -e branches -e branch-misses -r 3 sleep 1

perf stat with stream_count=1:

#+BEGIN_EXAMPLE
$ sudo ~/perf stat -C0 -e cycles -e  instructions \
                       -e cache-references -e cache-misses \
                       -e branches -e branch-misses -r 3 sleep 1

 Performance counter stats for 'CPU(s) 0' (3 runs):

  3,804,456,431  cycles                                        ( +-  0.00% )
  9,038,093,597  instructions      #2.38  insn per cycle       ( +-  0.02% )
     45,804,413  cache-references                              ( +-  0.01% )
            872  cache-misses      #0.002 % of all cache refs  ( +- 50.63% )
  1,653,739,128  branches                                      ( +-  0.02% )
      1,601,127  branch-misses     #0.10% of all branches      ( +-  0.19% )
#+END_EXAMPLE


perf stat with stream_count=1:

#+BEGIN_EXAMPLE
 Performance counter stats for 'CPU(s) 0' (3 runs):

  3,804,676,127  cycles                                        ( +-  0.00% )
  6,254,535,601  instructions      #1.64  insn per cycle       ( +-  0.07% )
     83,942,715  cache-references                              ( +-  0.05% )
         56,915  cache-misses      #0.068 % of all cache refs  ( +- 52.14% )
  1,157,902,407  branches                                      ( +-  0.07% )
     13,982,160  branch-misses     #1.21% of all branches      ( +-  0.05% )
#+END_EXAMPLE


Perf report shows that the issues is in fib_table_lookup(), and a
closer look show that this is likely the prefix backtracking in the
FIB lookup that is causing this.

#+BEGIN_EXAMPLE
Samples: 3M of event 'cycles:ppp', Event count (approx.): 3091862629036
  Overhead  CPU  Command       Shared Object     Symbol
+   34.29%  000  ksoftirqd/0   [kernel.vmlinux]  [k] fib_table_lookup
+    6.00%  000  ksoftirqd/0   [kernel.vmlinux]  [k] bpf_ipv4_fib_lookup
+    4.20%  000  ksoftirqd/0   [mlx5_core]       [k] mlx5e_handle_rx_cqe_mpwrq
+    3.96%  000  ksoftirqd/0   [mlx5_core]       [k] mlx5e_skb_from_cqe_mpwrq_linear
+    3.66%  000  ksoftirqd/0   [kernel.vmlinux]  [k] __xdp_return
+    3.40%  000  ksoftirqd/0   [mlx5_core]       [k] mlx5e_xdp_handle
+    3.07%  000  ksoftirqd/0   [mlx5_core]       [k] mlx5e_xmit_xdp_frame
+    2.66%  000  ksoftirqd/0   [mlx5_core]       [k] mlx5e_xdp_xmit
+    2.56%  000  ksoftirqd/0   [kernel.vmlinux]  [k] swiotlb_map_page
+    2.46%  000  ksoftirqd/0   [kernel.vmlinux]  [k] xdp_do_redirect
+    2.43%  000  ksoftirqd/0   [kernel.vmlinux]  [k] __page_pool_put_page
+    2.38%  000  ksoftirqd/0   [kernel.vmlinux]  [k] dev_map_enqueue
+    2.37%  000  ksoftirqd/0   [mlx5_core]       [k] mlx5e_post_rx_mpwqes
+    2.36%  000  ksoftirqd/0   [mlx5_core]       [k] mlx5e_poll_xdpsq_cq
+    1.76%  000  ksoftirqd/0   [kernel.vmlinux]  [k] page_pool_alloc_pages
+    1.72%  000  ksoftirqd/0   [mlx5_core]       [k] mlx5e_poll_rx_cq
+    1.21%  000  ksoftirqd/0   [kernel.vmlinux]  [k] ip_mtu_from_fib_result
+    1.05%  000  ksoftirqd/0   [kernel.vmlinux]  [k] bpf_xdp_fib_lookup
#+END_EXAMPLE


If a change how the dst-IPs are generated, then the performance is
significantly improved.  Thus, the T-rex test script is really good to
exercise the FIB lookup.


*** Recompiled kernel with CONFIG_IP_FIB_TRIE_STATS

This is an attempt to investigate why the FIB lookup code is
performing worse with the full Internet routing table, by looking at
the stats avail via /proc/net/fib_triestat (when kernel is compiled
with CONFIG_IP_FIB_TRIE_STATS).

#+BEGIN_EXAMPLE
Ethtool(mlx5p1  ) stat:      3581088 (      3,581,088) <= rx_xdp_redirect /sec
Ethtool(mlx5p2  ) stat:      3583691 (      3,583,691) <= tx0_xdp_xmit /sec
#+END_EXAMPLE

Fib stats from /proc/net/fib_triestat using the tool mmwatch from:
 https://github.com/cloudflare/cloudflare-blog/tree/master/2017-06-29-ssdp

#+BEGIN_EXAMPLE
Every 2.0s: cat /proc/net/fib_triestat		2018-06-21 20:33:00.739115

Basic info: size of leaf: 48 bytes, size of tnode: 40 bytes.
Main:
	Aver depth:     2.25
	Max depth:      6
	Leaves:         696323
	Prefixes:       752177
	Internal nodes: 136422
	  1: 161  2: 85300  3: 25308  4: 12140  5: 7833  6: 5405  7: 273  9: 1  18: 1
	Pointers: 1632402
Null ptrs: 799658
Total size: 91857  kB

Counters:
---------
gets =     3.6m/s
backtracks =  55.2k/s
semantic match passed =     3.6m/s
semantic match miss =     2.6m/s
null node hit=    10.9m/s
skipped node resize = 0

Local:
	Aver depth:     2.25
	Max depth:      6
	Leaves:         696323
	Prefixes:       752177
	Internal nodes: 136422
	  1: 161  2: 85300  3: 25308  4: 12140  5: 7833  6: 5405  7: 273  9: 1  18: 1
	Pointers: 1632402
Null ptrs: 799658
Total size: 91857  kB

Counters:
---------
gets =     3.6m/s
backtracks =  55.2k/s
semantic match passed =     3.6m/s
semantic match miss =     2.6m/s
null node hit=    10.9m/s
skipped node resize = 0
#+END_EXAMPLE

From above stats, the strange observation is the 10.9M/s "null node
hit" lookups.  This need further investigation, but could possibly
explain why we are seeing more branch-miss predictions.
